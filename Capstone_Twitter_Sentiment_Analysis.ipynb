{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Capstone Twitter Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhanieArdiyan/ASE2306-Capstone-Project/blob/main/Capstone_Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93cFhRHhrNgT",
        "outputId": "3a4ee35d-6599-4014-93ea-ff683cc58a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install stopwords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stopwords\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/a2/d4eb1d5e609fa0b1d59e929db8eb9ae540f8b2d6db3a4ba26f713e81af15/stopwords-0.1.3.tar.gz (41kB)\n",
            "\r\u001b[K     |███████▉                        | 10kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 30kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 40kB 3.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: stopwords\n",
            "  Building wheel for stopwords (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stopwords: filename=stopwords-0.1.3-py2.py3-none-any.whl size=37286 sha256=56328952168d9219ea602afdc510279ea3c8089eff809edfec1953c28f949cab\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/fa/c7/c4c5111e658f5c58465d948165dc3395a3c10ff57f4cd20356\n",
            "Successfully built stopwords\n",
            "Installing collected packages: stopwords\n",
            "Successfully installed stopwords-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85kbYjbt2wVr",
        "outputId": "fe0c34b2-af00-4784-8d2d-4bcf2f6c74fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ssv-DNGS_vK"
      },
      "source": [
        "#START INITIALISATION\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import sys, tweepy\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "def percentage(part, whole):\n",
        "\treturn 100 * float(part)/float(whole)\n",
        "\n",
        "consumer_key = \"duCD34AKM0yj2FJGe1vOomirc\"\n",
        "consumer_secret = \"aKkRF84oLs3pGm5gJzPavojoI23WZXMxrv4FWGZMKWnnSCoF1r\"\n",
        "access_token = \"1284696550740901893-xwi5gsHUdnwnD1ztZbSyVdhe6ZWdev\"\n",
        "access_token_secret = \"y8yNJJIxXtKiUgeY6kh8xWNSGEssNdCAdTIwQqnqR57yk\"\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
        "auth.set_access_token(access_token, access_token_secret) \n",
        "api = tweepy.API(auth, wait_on_rate_limit=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "803UPqaDluEa"
      },
      "source": [
        "#EXTRACTING TWEETS\n",
        "keyWord = input(\"Enter the keyword you are searching for (skip if NA):  \") #search by keyword\n",
        "UserName = input(\"Enter the Twitter username (excluding @, skip if NA): \") #search by username\n",
        "inputString = keyWord +\" from:\" + UserName\n",
        "\n",
        "public_tweets = api.search(q=inputString, count=1000, lang = \"en\")\n",
        "\n",
        "i = 1\n",
        "for tweet in public_tweets:\n",
        "  print ('\\n')\n",
        "  print ((str(i) + ') ' + tweet.text))\n",
        "  print (('Tweeted by') + ': ' + str(tweet.user.screen_name))\n",
        "  print (('Tweeted at') + ': ' + str(tweet.user.location))\n",
        "  print (('Tweeted on') + ': ' + str(tweet.created_at))\n",
        "  i=i+1\n",
        "  # print(tweet.text)\n",
        "\n",
        "#IF I want to print the latest 5 tweets from the account\n",
        "# print(\"Show the 5 recent tweets: \\n\")\n",
        "# i = 1\n",
        "# for tweet in posts [0:5]:\n",
        "#     print ((str(i) + ') ' + tweet.full_text + '\\n'))\n",
        "#     i = i + 1\n",
        "\n",
        "#Creating a dataframe with a column called Tweets\n",
        "df = pd.DataFrame ([tweet.text for tweet in public_tweets], columns = ['Tweets'])\n",
        "\n",
        "#Show first 5 rows of data\n",
        "df.head()\n",
        "\n",
        "#function to clean the tweets\n",
        "def cleanTxt(text):\n",
        "\ttext = re.sub(r'@[A-Za-z0-9]+', '', text) #removes @mentions\n",
        "\ttext = re.sub(r'#', '', text) #removes the '#'\n",
        "\ttext = re.sub(r'RT[\\s+]+', '', text) #removing RT\n",
        "\ttext = re.sub(r'https?:\\/\\/\\S+', '', text) #removes hyperlinks\n",
        "\treturn text\n",
        "\n",
        "#cleaning the text\n",
        "df['Tweets'] = df['Tweets'].apply(cleanTxt)\n",
        "\n",
        "#Show the cleaned text\n",
        "df\n",
        "\n",
        "#Subjectivity function\n",
        "def getSubjectivity(text):\n",
        "\treturn TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "#Polarity function\n",
        "def getPolarity(text):\n",
        "\treturn TextBlob(text).sentiment.polarity\n",
        "\n",
        "#Creating columns for Subjectivity and Polarity\n",
        "df ['Subjectivity'] = df['Tweets'].apply(getSubjectivity)\n",
        "df ['Polarity'] = df['Tweets'].apply(getPolarity)\n",
        "\n",
        "#Show the new dataframe with the new columns\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3z6ULaPS4mz"
      },
      "source": [
        "# #Plotting WORDCLOUD\n",
        "allWords = ' '.join([twts for twts in df['Tweets']])\n",
        "wordcloud = WordCloud(width = 5000, height = 300, random_state = 21, max_font_size = 100).generate(allWords)\n",
        "\n",
        "plt.imshow(WordCloud, interpolation = \"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uFmQ7rsOu-L"
      },
      "source": [
        "#Computing the negative, neutral, positive analysis\n",
        "def getAnalysis (score):\n",
        "\tif score < 0:\n",
        "\t\treturn 'Negative'\n",
        "\telif score == 0:\n",
        "\t\treturn 'Neutral'\n",
        "\telse:\n",
        "\t\treturn 'Positive'\n",
        "\n",
        "df['Analysis'] = df['Polarity'].apply(getAnalysis)\n",
        "df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsaMCganO8wI"
      },
      "source": [
        "#To seperate and print POSITIVE tweets\n",
        "j = 1\n",
        "sortedDF = df.sort_values(by=['Polarity'])\n",
        "for i in range (0, sortedDF.shape[0]):\n",
        "\tif (sortedDF['Analysis'][i] == 'Positive'):\n",
        "\t\tprint (str(j) + ') '+sortedDF['Tweets'][i])\n",
        "\t\tprint()\n",
        "\t\tj = j + 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gpq5V-kO3TR"
      },
      "source": [
        "#To seperate and print NEGATIVE tweets\n",
        "j = 1\n",
        "sortedDF = df.sort_values(by=['Polarity'], ascending='False')\n",
        "for i in range(0, sortedDF.shape[0]):\n",
        "  if (sortedDF['Analysis'][i] == 'Negative'):\n",
        "    print (str(j) + ') '+sortedDF['Tweets'][i])\n",
        "    print()\n",
        "    j = j + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1JNjhZIQ78a"
      },
      "source": [
        "#POSITIVE TWEETS in %\n",
        "ptweets = df[df.Analysis == 'Positive']\n",
        "ptweets = ptweets['Tweets']\n",
        "\n",
        "positive = round ( (ptweets.shape[0]/df.shape[0]) * 100, 1)\n",
        "positive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwO7XeziRr-B"
      },
      "source": [
        "#NEGATIVE TWEETS in %\n",
        "ntweets = df[df.Analysis == 'Negative']\n",
        "ntweets = ntweets['Tweets']\n",
        "\n",
        "negative = round ( (ntweets.shape[0]/df.shape[0]) * 100, 1)\n",
        "negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX4o2CGRqv7k"
      },
      "source": [
        "#NEUTRAL TWEETS in %\n",
        "100 - positive - negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_kNCZsyR3cg"
      },
      "source": [
        "#BAR GRAPH - POSITIVE, NEGATIVE, NEUTRAL\n",
        "df['Analysis'].value_counts()\n",
        "\n",
        "#plot and visualise the counts\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Counts')\n",
        "df['Analysis'].value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAcuvzzPQF9v"
      },
      "source": [
        "#SCATTER PLOT - POLARITY AND SUBJECTIVITY\n",
        "plt.figure(figsize = (8,6))\n",
        "for i in range (0,df.shape[0]):\n",
        "  plt.scatter(df['Polarity'][i], df['Subjectivity'][i], color='Blue') #(x,y axis)\n",
        "\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOy8Sed20IU2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96vo_klsvtwz"
      },
      "source": [
        "#ATTEMPT 1 TO MEASURE ACCURACY\n",
        "#METHOD: USING LSTM & RNN\n",
        "#link: https://analyticsindiamag.com/how-to-implement-lstm-rnn-network-for-sentiment-analysis/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_jC2AqDVYXo"
      },
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# df.to_csv('insertcsvfilehere.csv')\n",
        "# files.download('insertcsvfilehere.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi_jdy3xrafS"
      },
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from keras.preprocessing.text import Tokenizer\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# from keras.utils.np_utils import to_categorical\n",
        "# import re\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# df.to_csv('f1.csv')\n",
        "# files.download('f1.csv')\n",
        "\n",
        "# print(df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-zhgDIurflb"
      },
      "source": [
        "# new_df = df[['text','sentiment']]\n",
        "\n",
        "# print(data.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAbbItdRrjQN"
      },
      "source": [
        "# new_df = new_df[new_df.sentiment != \"Neutral\"]\n",
        "# new_df['text'] = new_df['text'].str.lower()\n",
        "# new_df['text'] = new_df['text'].re.sub('[^a-zA-z0-9\\s]')\n",
        "\n",
        "# tokenizer = Tokenizer(num_words=1500, split=' ')\n",
        "# tokenizer.fit_on_texts(data['text'].values)\n",
        "# X = tokenizer.texts_to_sequences(new_df['text'])\n",
        "# X = pad_sequences(X)\n",
        "\n",
        "# embed_dim = 128\n",
        "# lstm_out = 196\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(vocabSize, embed_dim,input_length = 28)) \n",
        "\n",
        "# model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "# model.add(Dense(2,activation='softmax'))\n",
        "# model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS3Pk_fPryUg"
      },
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# Le = LabelEncoder()\n",
        "# y = Le.fit_transform(new_df['sentiment'])\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15, random_state = 42)\n",
        "# model.fit(X_train, Y_train,validation_data = (X_test,y_test),epochs = 10, batch_size=32)\n",
        "\n",
        "# model.evaluate(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWE-4zbmr9J6"
      },
      "source": [
        "# print(\"Prediction: \",model.predict_classes(X_test[5:10]))\n",
        "# print(\"Actual: \\n\",y_test[5:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzzyShWN0C5w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyGMA6dCxe-i"
      },
      "source": [
        "#ATTEMPT 2 TO MEASURE ACCURACY\n",
        "#METHOD: Training model by RandomForestClassifier algorithm\n",
        "#link: https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98saclPJEZ3U"
      },
      "source": [
        "# import matplotlib.pyplot as plt \n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# processed_features = []\n",
        "# features = df['Tweets']\n",
        "\n",
        "\n",
        "# for sentence in range(0, len(features)):\n",
        "#     # Remove all the special characters\n",
        "#     processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n",
        "\n",
        "     # remove all single characters\n",
        "#     processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
        "\n",
        "#     # Remove single characters from the start\n",
        "#     processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n",
        "\n",
        "#     # Substituting multiple spaces with single space\n",
        "#     processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
        "\n",
        "#     # Removing prefixed 'b'\n",
        "#     processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
        "\n",
        "#     # Converting to Lowercase\n",
        "#     processed_feature = processed_feature.lower()\n",
        "\n",
        "#     processed_features.append(processed_feature)\n",
        "\n",
        "# processed_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prvDBmYnLnKq"
      },
      "source": [
        "# from nltk.corpus import stopwords\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
        "# processed_features = vectorizer.fit_transform(processed_features).toarray()\n",
        "\n",
        "# #max_features = most frequently occuring words to create a bag of words feature vector\n",
        "# #max_df = specifies to only using words that occur in a maximum of 80% of the docs\n",
        "# #min_df = similar, include words that occur in at least 7 docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieDjC9BJNJwO"
      },
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(processed_features, df, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJco-l21Ng87"
      },
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "# text_classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCreftKpNiFA"
      },
      "source": [
        "# predictions = text_classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KHey9XtNkuk"
      },
      "source": [
        "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# print(confusion_matrix(y_test,predictions))\n",
        "# print(classification_report(y_test,predictions))\n",
        "# print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpV12ILh06rk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Su7rAOZ06Zr"
      },
      "source": [
        "#ATTEMPT 3 TO MEASURE ACCURACY\n",
        "#METHOD: Implementing SVM with Scikit-Learn\n",
        "#link: https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1Vm6czz1QJD"
      },
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "# bankdata = pd.read_csv(\"bill_authentication.csv\")\n",
        "# bankdata.shape\n",
        "# bankdata.head()\n",
        "\n",
        "# X = bankdata.drop('Class', axis=1)\n",
        "# y = bankdata['Class']\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "\n",
        "# from sklearn.svm import SVC\n",
        "# svclassifier = SVC(kernel='linear')\n",
        "# svclassifier.fit(X_train, y_train)\n",
        "\n",
        "# y_pred = svclassifier.predict(X_test)\n",
        "\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "# print(confusion_matrix(y_test,y_pred))\n",
        "# print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}